\section{Embedded Submanifolds of a Linear Space}


\begin{frame}[allowframebreaks]{Embedded Submanifolds of a Linear Space}

  A subset $\M$ of $\RR^d$ is an \textcolor{gustave}{embedded manifold of dimension $n$}  if for each point $x\in\M$, there exists a neighborhood in $\M$ of $x$ (i.e. $\M\cap U$ for some open set $U\subset \RR^d$ containing $x$) that is \textcolor{gray}{approximate} to an open subset of $\RR^n$.

  \begin{figure}
    \centering
    % https://www.geogebra.org/classic/r7dwyts9
    \begin{minipage}{0.4\textwidth}
      \centering
      \includegraphics[width=0.7\textwidth]{img/sphere.png}
      \caption*{A sphere is a manifold}
    \end{minipage}
    \hspace{0.5cm}
    % https://www.geogebra.org/classic/gjmghdym
    \begin{minipage}{0.4\textwidth}
      \centering
      \includegraphics[width=0.7\textwidth]{img/cone.png}
      \caption*{A cone is not a manifold because every neighborhood of the tip (in red) cannot be approximated by a plane}
    \end{minipage}
  \end{figure}


  % \textcolor{red}{
  %   Intuition for why $\RR^{d-n}$: because we can think of the manifold as being defined by d-n constraints in $\RR^d$.
  % }

  We consider \textcolor{gustave}{smooth submanifolds}: for each $x\in\M$, there exists an open set $U\subset \RR^d$ containing $x$ and a smooth map $h: U \to \RR^{d-n}$ such that $M\cap U = h^{-1}(\{0\})$.

  By being approximate to $\RR^n$, we mean that for any direction $v\in\RR^d$ that is a \textcolor{gray}{tangent vector} to $\M$ at $x$, we have
  $$h(x+tv) = o(t).$$

  We rely on curves for the definition of tangent vectors (and also later definitions).

  \begin{mydefinition}[Tangent space]
  The tangent space $T_x\M$ at a point $x\in\M$ is the set of all tangent vectors to $\M$ at $x$ i.e.
  $$T_x\M = \{\gamma'(0) \,|\, \gamma:(-\epsilon,\epsilon) \to \M \in \C^\infty(-\epsilon,\epsilon), \gamma(0) = x \}.$$
  \end{mydefinition}

  Now we can use Taylor expansion to write
  $$h(x+tv) = h(x) + t \mathrm{D}h(x)[v] + o(t) = t \mathrm{D}h(x)[v] + o(t).$$

  \begin{myproposition}
    For every $x\in \M$, we have $$T_x\M \subseteq \text{ker}(\mathrm{D}h(x)).$$
  \end{myproposition}

  \begin{proof}
    Let $v\in T_x\M$, then there is a smooth $\gamma$ such that $\gamma(0) = x$ and $\gamma'(0) = v$. Consider $g(t) = h(\gamma(t))$. Since $\gamma(t)\in \M$ for all $t$, we have $g(t) = 0$ for all $t$. Thus, $g'(0) = 0$. By chain rule,
    $$g'(0) = \mathrm{D}h(\gamma(0))[\gamma'(0)] = \mathrm{D}h(x)[v] = 0.$$
    Hence, $v\in \text{ker}(\mathrm{D}h(x))$.
  \end{proof}

  \framebreak

  By the rank-nullity theorem, we have $\text{rank}(\mathrm{D}h(x)) \leq d-n$. Thus,
  $$\text{dim}(\text{ker}(\mathrm{D}h(x))) = d - \text{rank}(\mathrm{D}h(x)) \geq n.$$
  On the other hand, $\text{dim}(T_x\M) \leq n$.

  Therefore, if there is $x\in\M$ such that $\text{rank}(\mathrm{D}h(x)) < d-n$, then
  $$T_x\M \subsetneq \text{ker}(\mathrm{D}h(x)).$$
  That means there are vectors in $\text{ker}(\mathrm{D}h(x))$ that are not tangent to $\M$ at $x$ but can be used to approximate $\M$ near $x$ (we want to avoid this situation).

  For example, define cone shown previously by $h(x,y,z) = z^2 - x^2 - y^2$. At the tip $(0,0,0)$, we have $\mathrm{D}h(0,0,0) = [0 \; 0 \; 0]$ and $\ker \mathrm{D}h(0,0,0) = \RR^3$. So any vector in $\RR^3$ can be used to approximate the cone near the tip. But $v = (0,0,1)$ is not a tangent vector.


  \framebreak
  \begin{mydefinition}
  A subset $\M$ of $\RR^d$ is an embedded submanifold of dimension $n$ if for each $x\in\M$, there exists an open set $U\subset \RR^d$ containing $x$ and a smooth map $h: U \to \RR^{d-n}$ such that $$M\cap U = h^{-1}(\{0\}) \text{ and } \forall x\in \M\cap U, \rank\,\mathrm{D}h(x) = d-n.$$
  \end{mydefinition}

  \begin{myproposition}
    Using the convention that $\RR^0 = \{0\}$, every open subset of $\RR^d$ is a $d$-dimensional embedded submanifold of $\RR^d$.
  \end{myproposition}

  \textcolor{red}{We may add the theorem that this definition is equivalent to the diffeomorphism definition.}
\end{frame}

\begin{frame}{Embedded Submanifolds of a Linear Space}
    
\end{frame}



\begin{frame}{Examples of Optimization on Manifolds: Sphere}
  Let $A\in \mathbb R^{n\times n}$ be a symmetric matrix where $A^\top = A$. By the spectral theorem, $A$ will admit $n$ real eigen-values $\lambda_1
  \leq...\le\lambda_n$ and corresponding real, orthonormal eigenvectors $v_1,...,v_n \in \mathbb R^n$.

  Now let $\mathbb R^n_*$ denote the set of nonzero vectors in $\mathbb R^n$, or we can say that it is an open submanifold of $\mathbb R^n$. Then we have the Rayleigh quotient 
  \begin{align*}
      r: \mathbb R^n_* \to \mathbb R: x\mapsto r(x) = \frac{\langle x, Ax\rangle}{\langle x, x\rangle},
  \end{align*}
  attains its extreme values when x is aligned with $\pm v_1$ or $\pm v_n$, and that the corresponding value of the quotient is $\lambda_1$ or $\lambda_n$. Now we are interested in the smallest eigen-value $\lambda_1$, or we prefer to solve the following optimization problem:
  \begin{align*}
      \min_{x\in \mathbb R^n_*} \frac{\langle x, Ax\rangle}{\langle x, x\rangle}.
  \end{align*}
\end{frame}

\begin{frame}{Examples of Optimization on Manifolds: Sphere}
    Since the Rayleigh quotient is invariant to scaling, that is, since $r(\alpha x) = r(x)$ for all nonzero real $\alpha$, we can restrict attention to unit vectors without loss of generality, turning the problem into optimization over the unit sphere in $\mathbb R^n$:
    \begin{align*}
        S^{n-1} = \{x\in \mathbb R^n: \|x\| = 1\}.
    \end{align*}
    This is an embedded submanifold of $\mathbb R^n$. Our problem becomes:
    \begin{align*}
        \min_{x\in S^{-1}} \langle x, Ax\rangle,
    \end{align*}
    which will not change the minimum.
\end{frame}

\begin{frame}{Examples of Optimization on Manifolds: Stiefel manifold}
    Let $X \in \mathbb{R}^{m \times n}$ be a data matrix whose columns are centered data points.
We consider the problem of finding a $k$-dimensional subspace that maximizes the projected variance.

This can be formulated as the optimization problem
\begin{align*}
\max_{U \in \mathbb{R}^{n \times k}} \; \mathrm{tr}(U^\top X^\top X U)
\quad \text{subject to} \quad
U^\top U = I_k.
\end{align*}

The constraint $U^\top U = I_k$ defines the \emph{Stiefel manifold}
\begin{align*}
\mathrm{St}(k,n) = \{ U \in \mathbb{R}^{n \times k} : U^\top U = I_k \}.
\end{align*}

Thus, the problem becomes an optimization problem on the Stiefel manifold:
\begin{align*}
\max_{U \in \mathrm{St}(k,n)} \; \mathrm{tr}(U^\top X^\top X U).
\end{align*}

The optimal solution is given by the matrix $U$ whose columns are the eigenvectors
corresponding to the $k$ largest eigenvalues of $X^\top X$.

\end{frame}

% \begin{frame}{Examples of Optimization on Manifolds}
    
% \end{frame} 